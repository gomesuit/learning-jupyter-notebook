{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ７章 機械学習モデルを構築する１０本ノック\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６１：フォルダ生成をして機械学習用データを読み込もう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '../../downloads/ml_100knock/chapter-7/data/'\n",
    "input_dir = os.path.join(data_dir, '0_input')\n",
    "output_dir = os.path.join(data_dir, '1_output')\n",
    "os.makedirs(input_dir,exist_ok=True)\n",
    "os.makedirs(output_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ml_data_file = 'ml_base_data.csv'\n",
    "ml_data = pd.read_csv(os.path.join(input_dir, ml_data_file))\n",
    "ml_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６２：カテゴリカル変数の対応をしよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data = pd.get_dummies(ml_data['store_name'], prefix='store' ,prefix_sep='_')\n",
    "display(category_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del category_data['store_麻生店']\n",
    "del ml_data['year_month']\n",
    "del ml_data['store_name']\n",
    "ml_data = pd.concat([ml_data, category_data],axis=1)\n",
    "ml_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６３：学習データとテストデータを分割しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(ml_data, test_size=0.3, random_state=0)\n",
    "print(f'Train：{len(train_data)}件/ Test:{len(test_data)}')\n",
    "print(f'Weekday Train0：{len(train_data.loc[train_data[\"y_weekday\"]==0])}件')\n",
    "print(f'Weekday Train1：{len(train_data.loc[train_data[\"y_weekday\"]==1])}件')\n",
    "print(f'Weekday Test0：{len(test_data.loc[test_data[\"y_weekday\"]==0])}件')\n",
    "print(f'Weekday Test1：{len(test_data.loc[test_data[\"y_weekday\"]==1])}件')\n",
    "\n",
    "print(f'Weekend Train0：{len(train_data.loc[train_data[\"y_weekend\"]==0])}件')\n",
    "print(f'Weekend Train1：{len(train_data.loc[train_data[\"y_weekend\"]==1])}件')\n",
    "print(f'Weekend Test0：{len(test_data.loc[test_data[\"y_weekend\"]==0])}件')\n",
    "print(f'Weekend Test1：{len(test_data.loc[test_data[\"y_weekend\"]==1])}件')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６４：１つのモデルを構築しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = list(train_data.columns)\n",
    "X_cols.remove('y_weekday')\n",
    "X_cols.remove('y_weekend')\n",
    "target_y = 'y_weekday'\n",
    "y_train = train_data[target_y]\n",
    "X_train = train_data[X_cols]\n",
    "y_test = test_data[target_y]\n",
    "X_test = test_data[X_cols]\n",
    "display(y_train.head(3))\n",
    "display(X_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６５：評価を実施してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score,confusion_matrix\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "print(f'【正解率】Train：{round(acc_train,2)} Test：{round(acc_test, 2)}')\n",
    "print(f'【F値】Train：{round(f1_train,2)} Test：{round(f1_test, 2)}')\n",
    "print(f'【再現率】Train：{round(recall_train,2)} Test：{round(recall_test, 2)}')\n",
    "print(f'【適合率】Train：{round(precision_train,2)} Test：{round(precision_test, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred_train).ravel()\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "print(f'【混同行列】Train：{tn_train}, {fp_train}, {fn_train}, {tp_train}')\n",
    "print(f'【混同行列】Test：{tn_test}, {fp_test}, {fn_test}, {tp_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = pd.DataFrame({'DataCategory':['train'],'acc':[acc_train],'f1':[f1_train], \n",
    "                            'recall':[recall_train],'precision':[precision_train],\n",
    "                            'tp':[tp_train],'fn':[fn_train],'fp':[fp_train],'tn':[tn_train]})\n",
    "score_test = pd.DataFrame({'DataCategory':['test'], 'acc':[acc_test],'f1':[f1_test], \n",
    "                            'recall':[recall_test],'precision':[precision_test],\n",
    "                            'tp':[tp_test],'fn':[fn_test],'fp':[fp_test],'tn':[tn_test]})\n",
    "score = pd.concat([score_train,score_test], ignore_index=True)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６６：モデルの重要度を確認してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'cols':X_train.columns, 'importance':model.feature_importances_})\n",
    "importance = importance.sort_values('importance', ascending=False)\n",
    "importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６７：モデル構築から評価までを関数化しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_eval(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred_train).ravel()\n",
    "    tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "    score_train = pd.DataFrame({'DataCategory':['train'],'acc':[acc_train],'f1':[f1_train], \n",
    "                                'recall':[recall_train],'precision':[precision_train],\n",
    "                                'tp':[tp_train],'fn':[fn_train],'fp':[fp_train],'tn':[tn_train]})\n",
    "    score_test = pd.DataFrame({'DataCategory':['test'], 'acc':[acc_test],'f1':[f1_test], \n",
    "                                'recall':[recall_test],'precision':[precision_test],\n",
    "                                'tp':[tp_test],'fn':[fn_test],'fp':[fp_test],'tn':[tn_test]})\n",
    "    score = pd.concat([score_train,score_test], ignore_index=True)\n",
    "    importance = pd.DataFrame({'cols':X_train.columns, 'importance':model.feature_importances_})\n",
    "    importance = importance.sort_values('importance', ascending=False)\n",
    "    cols = pd.DataFrame({'X_cols':X_train.columns})\n",
    "    display(score)\n",
    "    return score, importance, model, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "score, importance, model, cols = make_model_and_eval(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６８：モデルファイルや評価結果を出力しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "target_output_dir_name = 'results_' + now\n",
    "target_output_dir = os.path.join(output_dir, target_output_dir_name)\n",
    "os.makedirs(target_output_dir, exist_ok=True)\n",
    "print(target_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = 'score.csv'\n",
    "importance_name = 'importance.csv'\n",
    "cols_name = 'X_cols.csv'\n",
    "model_nema = 'model.pickle'\n",
    "score_path = os.path.join(target_output_dir, score_name)\n",
    "importance_path = os.path.join(target_output_dir, importance_name)\n",
    "cols_path = os.path.join(target_output_dir, cols_name)\n",
    "model_path = os.path.join(target_output_dir, model_nema)\n",
    "\n",
    "score.to_csv(score_path, index=False)\n",
    "importance.to_csv(importance_path, index=False)\n",
    "cols.to_csv(cols_path, index=False)\n",
    "import pickle\n",
    "with open(model_path, mode='wb') as f:\n",
    "    pickle.dump(model, f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック６９：アルゴリズムを拡張して多角的な評価を実施しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "models = {'tree': DecisionTreeClassifier(random_state=0), \n",
    "          'RandomForest':RandomForestClassifier(random_state=0),\n",
    "          'GradientBoostingClassifier':GradientBoostingClassifier(random_state=0)}\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "target_output_dir_name = 'results_' + now\n",
    "target_output_dir = os.path.join(output_dir, target_output_dir_name)\n",
    "os.makedirs(target_output_dir, exist_ok=True)\n",
    "print(target_output_dir)\n",
    "\n",
    "score_all = []\n",
    "importance_all = []\n",
    "for model_name, model in models.items():\n",
    "    print(model_name)\n",
    "    score, importance, model, cols = make_model_and_eval(model, X_train, X_test, y_train, y_test)\n",
    "    score['model_name'] = model_name\n",
    "    importance['model_name'] = model_name\n",
    "    \n",
    "    model_nema = f'model_{model_name}.pickle'\n",
    "    model_path = os.path.join(target_output_dir, model_nema)\n",
    "    with open(model_path, mode='wb') as f:\n",
    "        pickle.dump(model, f, protocol=2)\n",
    "    score_all.append(score)\n",
    "    importance_all.append(importance)\n",
    "score_all = pd.concat(score_all, ignore_index=True)\n",
    "importance_all = pd.concat(importance_all, ignore_index=True)\n",
    "cols = pd.DataFrame({'X_cols':X_train.columns})\n",
    "\n",
    "score_name = 'score.csv'\n",
    "importance_name = 'importance.csv'\n",
    "cols_name = 'X_cols.csv'\n",
    "score_path = os.path.join(target_output_dir, score_name)\n",
    "importance_path = os.path.join(target_output_dir, importance_name)\n",
    "cols_path = os.path.join(target_output_dir, cols_name)\n",
    "score_all.to_csv(score_path, index=False)\n",
    "importance_all.to_csv(importance_path, index=False)\n",
    "cols.to_csv(cols_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノック７０：平日/休日モデルを一度で回せるようにしよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = list(train_data.columns)\n",
    "X_cols.remove('y_weekday')\n",
    "X_cols.remove('y_weekend')\n",
    "targets_y = ['y_weekday', 'y_weekend']\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "target_output_dir_name = 'results_' + now\n",
    "target_output_dir = os.path.join(output_dir, target_output_dir_name)\n",
    "os.makedirs(target_output_dir,exist_ok=True)\n",
    "print(target_output_dir)\n",
    "\n",
    "score_all = []\n",
    "importance_all = []\n",
    "\n",
    "for target_y in targets_y:\n",
    "    y_train = train_data[target_y]\n",
    "    X_train = train_data[X_cols]\n",
    "    y_test = test_data[target_y]\n",
    "    X_test = test_data[X_cols]\n",
    "    \n",
    "    models = {'tree': DecisionTreeClassifier(random_state=0), \n",
    "              'RandomForest':RandomForestClassifier(random_state=0),\n",
    "              'GradientBoosting':GradientBoostingClassifier(random_state=0)}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(model_name)\n",
    "        score, importance, model, cols = make_model_and_eval(model, X_train, X_test, y_train, y_test)\n",
    "        score['model_name'] = model_name\n",
    "        importance['model_name'] = model_name\n",
    "        score['model_target'] = target_y\n",
    "        importance['model_target'] = target_y\n",
    "\n",
    "        model_nema = f'model_{target_y}_{model_name}.pickle'\n",
    "        model_path = os.path.join(target_output_dir, model_nema)\n",
    "        with open(model_path, mode='wb') as f:\n",
    "            pickle.dump(model, f, protocol=2)\n",
    "        score_all.append(score)\n",
    "        importance_all.append(importance)\n",
    "        \n",
    "score_all = pd.concat(score_all, ignore_index=True)\n",
    "importance_all = pd.concat(importance_all, ignore_index=True)\n",
    "cols = pd.DataFrame({'X_cols':X_train.columns})\n",
    "\n",
    "score_name = 'score.csv'\n",
    "importance_name = 'importance.csv'\n",
    "cols_name = 'X_cols.csv'\n",
    "score_path = os.path.join(target_output_dir, score_name)\n",
    "importance_path = os.path.join(target_output_dir, importance_name)\n",
    "cols_path = os.path.join(target_output_dir, cols_name)\n",
    "score_all.to_csv(score_path, index=False)\n",
    "importance_all.to_csv(importance_path, index=False)\n",
    "cols.to_csv(cols_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_all.loc[score_all['model_target']=='y_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_all.loc[score_all['model_target']=='y_weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_all.loc[(importance_all['model_target']=='y_weekday')&\n",
    "                   (importance_all['model_name']=='GradientBoosting')].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
